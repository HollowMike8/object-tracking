{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HollowMike8/object-tracking-dlib/blob/main/single_object_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEy9i5RQEx8I"
      },
      "outputs": [],
      "source": [
        "!git\n",
        "!git init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o_1Mg_-FDUQ",
        "outputId": "f916de22-4b6e-40a3-d502-be5f32a5f831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'object-tracking-dlib'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 85 (delta 42), reused 32 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HollowMike8/object-tracking-dlib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wTAwFsNMMqb",
        "outputId": "59d32719-9b46-4938-ab72-afd5ea6469a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade imutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-TPdZjpFJap",
        "outputId": "716194e0-0f98-464c-f5ea-5a0bd1e0670e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/object-tracking-dlib\n"
          ]
        }
      ],
      "source": [
        "%cd object-tracking-dlib/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "caB1F7pQFSN6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "import imutils\n",
        "import datetime\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SXYQlSrdFgqA"
      },
      "outputs": [],
      "source": [
        "path_dir: str = r\"/content/object-tracking-dlib\"\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,path_dir)\n",
        "import single_object_config as soc\n",
        "from centroidtracker import CentroidTracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "yOLxxdhZKjRD"
      },
      "outputs": [],
      "source": [
        "# load input video (race.mp4), intitialize the writer, tracker \n",
        "vs = cv2.VideoCapture(os.path.join(soc.input_dir, \"race.mp4\"))\n",
        "\n",
        "tracker = None\n",
        "writer = None\n",
        "\n",
        "# intitialize the CentroidTracker, objects\n",
        "ct = CentroidTracker(maxDisappeared=40, maxDistance=40)\n",
        "objects = None\n",
        "\n",
        "# refresh rate for object detection (object detection every N frames)\n",
        "refresh_rate = 60\n",
        "\n",
        "# initiate totalFrames processed\n",
        "totalFrames = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRDZqfjIAT4E"
      },
      "source": [
        "# **mobilenet_ssd**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC8pGyoJJqUH"
      },
      "outputs": [],
      "source": [
        "# list of all the classes mobilenet_ssd was trained on\n",
        "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \n",
        "           \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \n",
        "           \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \n",
        "           \"tvmonitor\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldKIicqqJ9Eh"
      },
      "outputs": [],
      "source": [
        "# load the mobilenet_ssd caffe model\n",
        "prototxt_path = os.path.join(soc.cnn_caffe_dir , \"MobileNetSSD_deploy.prototxt\")\n",
        "model_path = os.path.join(soc.cnn_caffe_dir , \"MobileNetSSD_deploy.caffemodel\")\n",
        "\n",
        "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFL2Qy0xc_l0"
      },
      "outputs": [],
      "source": [
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# initialize list to capture the bounding box coordinates\n",
        "rects = []\n",
        "\n",
        "# loop over thr frames in the input video\n",
        "while True:\n",
        "  (grab, frame) = vs.read()\n",
        "\n",
        "  # to break out of loop after the end of video\n",
        "  if grab == False:\n",
        "    break\n",
        "\n",
        "  # convert from BGR to RGB for dlib tracker\n",
        "  frame = imutils.resize(frame, width=600)\n",
        "  img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # writing the video\n",
        "  if writer is None:\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "    writer = cv2.VideoWriter(os.path.join(soc.output_dir, \"race_dlib.avi\"), \n",
        "                             fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "  # object detection (for every N frames)\n",
        "  if totalFrames % refresh_rate == 0:\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 0.007843, (w, h), 127.5)\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # find the index of the detection with the largest confidence (single obj)\n",
        "    if objects == None and len(detections) > 0:\n",
        "      i = np.argmax(detections[0, 0, :, 2])\n",
        "      conf = detections[0, 0, i, 2]\n",
        "      label = CLASSES[int(detections[0, 0, i, 1])]\n",
        "\n",
        "    # find the index of the previouly existing single obj detection\n",
        "    else:\n",
        "      for i in range(0, detections.shape[2]):\n",
        "        conf = detections[0, 0, i, 2]\n",
        "        label = CLASSES[int(detections[0, 0, i, 1])]\n",
        "        # print(\"Label of progressive detection:%s\"% label)\n",
        "\n",
        "        temp = detections[0, 0, i, 3:7]*np.array([w, h, w, h])\n",
        "        (startX, startY, endX, endY) = temp.astype(\"int\")\n",
        "        # print(\"Rect of progressive detection:%s\"% temp)\n",
        "\n",
        "        if label == soc.label:\n",
        "          rects = []\n",
        "          objects_old = objects.copy()\n",
        "          rects.append((startX, startY, endX, endY))\n",
        "          objects = ct.update(rects)\n",
        "\n",
        "          # check the new rect is already detected/tracked single obj\n",
        "          if (objects_old[0] == objects[0]).all():\n",
        "            continue\n",
        "          else:\n",
        "            break\n",
        "\n",
        "        elif label == 'background':\n",
        "          label = soc.label\n",
        "\n",
        "    if conf > soc.thres_confidence and label == soc.label:\n",
        "      # compute the bounding box coordinates\n",
        "      box = detections[0, 0, i, 3:7]*np.array([w, h, w, h])\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "      # construct the dlib correlation tracker using bouding box coordinates\n",
        "      tracker = dlib.correlation_tracker()\n",
        "      rect = dlib.rectangle(startX, startY, endX, endY)\n",
        "      tracker.start_track(img, rect)\n",
        "\n",
        "      # draw the bouding box rectangle and label in the frame\n",
        "      cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
        "      cv2.putText(frame, label, (startX, startY-15), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                  0.45, (0, 255, 0), 2)\n",
        "      \n",
        "      # empty the rect list and update the centroid/centroids\n",
        "      rects = []\n",
        "      rects.append((startX, startY, endX, endY))\n",
        "      objects = ct.update(rects)\n",
        "  \n",
        "  # object tracking     \n",
        "  else:\n",
        "    tracker.update(img)\n",
        "    pos = tracker.get_position()\n",
        "\n",
        "    # unpack the position object\n",
        "    startX = int(pos.left())\n",
        "    startY = int(pos.top())\n",
        "    endX = int(pos.right())\n",
        "    endY = int(pos.bottom())\n",
        "\n",
        "    # draw the bouding box rectangle and label in the frame\n",
        "    cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
        "    cv2.putText(frame, label, (startX, startY-15), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                0.45, (0, 255, 0), 2)\n",
        "    \n",
        "    # empty the rect list and update the centroid/centroids\n",
        "    rects = []\n",
        "    rects.append((startX, startY, endX, endY))\n",
        "    objects = ct.update(rects)\n",
        "\n",
        "  # write the sketched frame     \n",
        "  if writer is not None:\n",
        "    writer.write(frame)\n",
        "\n",
        "  # show the output frame\n",
        "  cv2_imshow(frame)\n",
        "  key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "  # if the `q` key was pressed, break from the loop\n",
        "  if key == ord(\"q\"):\n",
        "    break\n",
        "\n",
        "  # update the totalFrames processed\n",
        "  totalFrames += 1\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "elapsed_time = (end_time-start_time).total_seconds()\n",
        "print(\"Elapsed time: {:.2f}\".format(elapsed_time))\n",
        "print(\"Approx. FPS: {:.2f}\".format(totalFrames/elapsed_time))\n",
        "\n",
        "# check to see if we need to release the video writer pointer\n",
        "if writer is not None:\n",
        "  writer.release()\n",
        "\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOBCx6baAbBT"
      },
      "source": [
        "# **yolo-coco**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiVN-9PqAd9U"
      },
      "outputs": [],
      "source": [
        "# download the files if required\n",
        "\n",
        "# change the dir to location where downloads save\n",
        "%cd /content/object-tracking-dlib/yolo-coco/\n",
        "\n",
        "!wget \"https://pjreddie.com/media/files/yolov3.weights\"\n",
        "\n",
        "!wget \"https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\"\n",
        "\n",
        "# !wget \"https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\"\n",
        "\n",
        "#change the dir back to root dir\n",
        "%cd /content/object-tracking-dlib/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zjF3uS34RjCI"
      },
      "outputs": [],
      "source": [
        "# load the COCO class labels for YOLO model\n",
        "labels_path = os.path.join(path_dir, \"yolo-coco\", \"coco.names\")\n",
        "labels = open(labels_path, \"r\").read().strip().split(\"\\n\")\n",
        "\n",
        "# assign random colours to all COCO class labels\n",
        "np.random.seed(42)\n",
        "colors = np.random.randint(0, 255, size=(len(labels), 3), dtype = \"uint8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cyEwOAgTBGfy"
      },
      "outputs": [],
      "source": [
        "# load the yolov3 model\n",
        "weightsPath = os.path.join(path_dir, \"yolo-coco\", \"yolov3.weights\")\n",
        "configPath = os.path.join(path_dir, \"yolo-coco\", \"yolov3.cfg\")\n",
        "\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UndYJGx2BZWN"
      },
      "outputs": [],
      "source": [
        "# initialize list to capture the bounding box coordinates\n",
        "rects = []\n",
        "\n",
        "# loop over thr frames in the input video\n",
        "while True:\n",
        "  (grab, frame) = vs.read()\n",
        "\n",
        "  # to break out of loop after the end of video\n",
        "  if grab == False:\n",
        "    break\n",
        "\n",
        "  # convert from BGR to RGB for dlib tracker\n",
        "  frame = imutils.resize(frame, width=600)\n",
        "  img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # writing the video\n",
        "  if writer is None:\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "    writer = cv2.VideoWriter(os.path.join(soc.output_dir, \"race_dlib_yolo.avi\"), \n",
        "                             fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "  # object detection (for every N frames)\n",
        "  # initialize lists to append the bounding boxes, confidences and class IDs\n",
        "  boxes = []\n",
        "  confidences = []\n",
        "  classIDs = []\n",
        "\n",
        "  if totalFrames % refresh_rate == 0:\n",
        "    (h, w) = frame.shape[:2]\n",
        "\n",
        "    # determine only *output* layer names we need from yolo (3 output layers)\n",
        "    layer_names = net.getLayerNames()\n",
        "    layer_names = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    # construct a blob from the input image and then perform a forward pass\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, \n",
        "                                 crop=False)\n",
        "    net.setInput(blob)\n",
        "    layerOutputs = net.forward(layer_names)\n",
        "\n",
        "    # loop over each layer of the outputs (3)\n",
        "    for output in layerOutputs:\n",
        "      # loop over the detections in each output\n",
        "      for detection in output:\n",
        "        scores = detection[5:]\n",
        "        classID = np.argmax(scores)\n",
        "        confidence = scores[classID]\n",
        "\n",
        "        # consider only predictions with confidence > threshold\n",
        "        if confidence > soc.yolo_thres_confidence:\n",
        "          # scale the bounding box parameters \n",
        "          box = detection[0:4] * np.array([w, h, w, h])\n",
        "          (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "          # find the corner points for cv2.rectangle\n",
        "          startX = int(centerX - (width/2))\n",
        "          startY = int(centerY - (height/2))\n",
        "          endX = int(centerX + (width/2))\n",
        "          endY = int(centerY + (height/2))\n",
        "\n",
        "          boxes.append([startX, startY, endX, endY])\n",
        "          confidences.append(float(confidence))\n",
        "          classIDs.append(classID)\n",
        "    \n",
        "    # apply non-max supression with threshold IoU= 0.3 and \n",
        "    # threshold confidence=soc.yolo_thres_confidence\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, soc.yolo_thres_confidence, 0.3)\n",
        "\n",
        "    # capture the all detections with class labels \"person\"\n",
        "    if len(idxs) > 0:\n",
        "      person_list = {confidences[i]:boxes[i] for i in idxs.flatten()\n",
        "                    if labels[classIDs[i]]==soc.label}\n",
        "      # print(person_list)\n",
        "\n",
        "    # find the detection with the largest confidence (single obj)\n",
        "    if objects == None and bool(person_list):\n",
        "      label = soc.label\n",
        "      max_conf = max(person_list.keys())\n",
        "      (startX, startY, endX, endY) = person_list[max_conf]\n",
        "\n",
        "    # find the index of the previouly existing single obj detection\n",
        "    elif objects != None and bool(person_list):\n",
        "      label = soc.label\n",
        "      rects_old = rects\n",
        "\n",
        "      for key in person_list.keys():\n",
        "        conf = key\n",
        "        (startX, startY, endX, endY) = person_list[conf]\n",
        "\n",
        "        rects = []\n",
        "        objects_old = objects.copy()\n",
        "        rects.append((startX, startY, endX, endY))\n",
        "        objects = ct.update(rects)\n",
        "\n",
        "        # check the new rect is already detected/tracked single obj\n",
        "        if (objects_old[0] == objects[0]).all():\n",
        "          (startX, startY, endX, endY) = rects_old[0]\n",
        "          continue\n",
        "        else:\n",
        "          break\n",
        "\n",
        "    # construct the dlib correlation tracker using bouding box coordinates\n",
        "    tracker = dlib.correlation_tracker()\n",
        "    rect = dlib.rectangle(startX, startY, endX, endY)\n",
        "    tracker.start_track(img, rect)\n",
        "\n",
        "    # draw the bouding box rectangle and label in the frame\n",
        "    cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
        "    cv2.putText(frame, label, (startX, startY-15), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                0.45, (0, 255, 0), 2)\n",
        "    \n",
        "    # empty the rect list and update the centroid/centroids\n",
        "    rects = []\n",
        "    rects.append((startX, startY, endX, endY))\n",
        "    objects = ct.update(rects)\n",
        "\n",
        "  # object tracking     \n",
        "  else:\n",
        "    tracker.update(img)\n",
        "    pos = tracker.get_position()\n",
        "\n",
        "    # unpack the position object\n",
        "    startX = int(pos.left())\n",
        "    startY = int(pos.top())\n",
        "    endX = int(pos.right())\n",
        "    endY = int(pos.bottom())\n",
        "\n",
        "    # draw the bouding box rectangle and label in the frame\n",
        "    cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
        "    cv2.putText(frame, label, (startX, startY-15), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                0.45, (0, 255, 0), 2)\n",
        "    \n",
        "    # empty the rect list and update the centroid/centroids\n",
        "    rects = []\n",
        "    rects.append((startX, startY, endX, endY))\n",
        "    objects = ct.update(rects)\n",
        "\n",
        "  # write the sketched frame     \n",
        "  if writer is not None:\n",
        "    writer.write(frame)\n",
        "\n",
        "  # show the output frame\n",
        "  cv2_imshow(frame)\n",
        "  key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "  # if the `q` key was pressed, break from the loop\n",
        "  if key == ord(\"q\"):\n",
        "    break\n",
        "\n",
        "  # update the totalFrames processed\n",
        "  totalFrames += 1\n",
        "\n",
        "# check to see if we need to release the video writer pointer\n",
        "if writer is not None:\n",
        "  writer.release()\n",
        "\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hanlag-dOzz"
      },
      "source": [
        "# **Notes**\n",
        "1. Object detection is performed once in every 60 frames\n",
        "\n",
        "2. Object detections (except initial) use additional criteria of checking if  the new detection is close to previous bounding box (from tracking)\n",
        "\n",
        "3. mobilenet_ssd fails in some detection steps (for refresh_rate = 30 frames) due to occlusion\n",
        "\n",
        "4. Unsuccessful detection steps are skipped and tracking is used as before\n",
        "  1. Tracking is re-initiated but with the last successful bounding box \n",
        "\n",
        "5. yolov3 fails in some detection steps (for refresh_rate = 30 frames) possibly due to occlusion"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "QRDZqfjIAT4E"
      ],
      "name": "single_object_tracking.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrE7Yy9EfTd0Anrq4/D34W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}